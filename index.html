<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?63e6df4d5793a7792212ff878286fbea";
    var s = document.getElementsByTagName("script")[0]; 
    s.parentNode.insertBefore(hm, s);
  })();
  </script>

  <title>Shangzhe Di - SJTU</title>
  
  <meta name="author" content="Shangzhe Di">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shangzhe Di &nbsp 狄尚哲</name>
              </p>
              <p>Hi, I am a third-year PhD candidate at Shanghai Jiao Tong University (SJTU), where I am fortunate to be advised by <a href="https://weidixie.github.io">Prof. Weidi Xie</a>. My research focuses on <b>video understanding</b> and <b>multimodal learning</b>, driven by a passion for exploring the unknowns in these fields.
              </p>
              <p> Before joining SJTU, I earned my master's and bachelor's degrees from Beihang University (BUAA). During this period, I explored video background music generation and visual object tracking under the guidance of <a href="https://colalab.net/people">Prof. Si Liu</a>.
              </p>
              <p style="color:rgb(232, 15, 15)">
                I’m always eager to connect, exchange ideas, and collaborate on innovative research.
                Please feel free to reach out!
              </p>
              <p style="text-align:center">
                <a href="mailto:shangzhe.di@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/cv_dsz.pdf">CV</a> &nbsp/&nbsp
                <a href="https://github.com/Becomebright/">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=qkO6rFQAAAAJ&hl=en">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:25%;max-width:40%">
              <a href="images/DiShangzhe.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/DiShangzhe.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- Education -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Education</heading>
              <div style="line-height:25px">
                <p>
                  <li>
                    PhD Student, <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a>, Apr. 2023 - Present<br />
                  <li>
                    M.Eng. in Computer Science, <a href="https://ev.buaa.edu.cn">Beihang University</a>, Sep. 2020 - Jan. 2023<br />
                  <li>
                    Exchange Student, <a href="https://www.in.tum.de/en/cover-page/">Technical University of Munich</a>, Apr. 2019 - Aug. 2019<br />
                  <li>
                    B.Eng. in Software Engineering, <a href="https://ev.buaa.edu.cn">Beihang University</a>, Sep. 2016 - Jun. 2020<br />
                </p>
              </div>
            </td>
          </tr>
        </table>

        <!-- Experience -->
<!--         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Experience</heading>
              <div style="line-height:25px">
                <p>
                  <li>
                    Research Intern (Foundation Models & Face Recognition), <a href="https://www.sensetime.com/en">SenseTime</a>, Aug. 2022 - Jan. 2023<br />
                  <li>
                    Research Intern (Music Generation & Retrieval), <a href="https://www.tencent.com/en-us/">Tencent</a>, Nov. 2021 - Feb. 2022<br />
                  <li>
                    Research Intern (Generative AI & Virtual Try-on), <a href="https://www.kwai.com">KwaiShou Y-Tech</a>, Oct. 2019 - Apr. 2021<br />
                </p>
              </div>
            </td>
          </tr>
        </table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- AoTD -->
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle;text-align:center;">
              <img src="images/AoTD.png" width="250">
            </td>
            <td width="70%" valign="middle">
              <a href="https://zhengrongz.github.io/AoTD/">
                <papertitle>Enhancing Video-LLM Reasoning via Agent-of-Thoughts Distillation</papertitle>
              </a>
              <br>
              Yudi Shi, <b>Shangzhe Di</b>, Qirui Chen, Weidi Xie
              <br>
              In <em>CVPR</em>, 2025.
              <br>
              <a href="https://arxiv.org/abs/2412.01694">paper</a> / <a href="https://zhengrongz.github.io/AoTD/">project page</a> / <a href="https://github.com/zhengrongz/AoTD">code</a> / <a href="data/aotd.bib">bibtex</a>
              <p>Distill multi-step reasoning and spatial-temporal understanding into a generative video-language model.</p>
            </td>
          </tr>

          <!-- ReKV -->
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle;text-align:center;">
              <img src="images/ReKV.png" width="250">
            </td>
            <td width="70%" valign="middle">
              <a href="">
                <papertitle>Streaming Video Question-Answering with In-context Video KV-Cache Retrieval</papertitle>
              </a>
              <br>
              <b>Shangzhe Di</b>, Zhelun Yu, et al.
              <br>
              In <em>ICLR</em>, 2025.
              <br>
              <a href="https://arxiv.org/abs/2503.00540">paper</a> / <a href="https://github.com/Becomebright/ReKV">code</a> / <a href="data/rekv.bib">bibtex</a>
              <p>A training-free approach enabling Video-LLMs for streaming video question-answering.</p>
            </td>
          </tr>

          <!-- MH-VidQA -->
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle;text-align:center;">
              <img src="images/MH-VidQA.jpeg" width="250">
            </td>
            <td width="70%" valign="middle">
              <a href="https://qirui-chen.github.io/MultiHop-EgoQA">
                <papertitle>Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos</papertitle>
              </a>
              <br>
              Qirui Chen, <b>Shangzhe Di</b>, Weidi Xie
              <br>
              In <em>AAAI</em>, 2025.
              <br>
              <a href="https://arxiv.org/abs/2408.14469">paper</a> / <a href="https://qirui-chen.github.io/MultiHop-EgoQA">project page</a> / <a href="https://github.com/qirui-chen/MultiHop-EgoQA">code</a> / <a href="data/mh-vidqa.bib">bibtex</a>
              <p>Pinpoint scattered visual evidence in long egocentric videos while responding to questions.</p>
            </td>
          </tr>

          <!-- GroundVQA -->
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle;text-align:center;">
              <img src="images/GroundVQA.png" width="250">
            </td>
            <td width="70%" valign="middle">
              <a href="GroundVQA/index.html">
                <papertitle>Grounded Question-Answering in Long Egocentric Videos</papertitle>
              </a>
              <br>
              <b>Shangzhe Di</b>, Weidi Xie
              <br>
              In <em>CVPR</em>, 2024.
              <br>
              <a href="https://arxiv.org/abs/2312.06505">paper</a> / <a href="GroundVQA/index.html">project page</a> / <a href="https://github.com/Becomebright/GroundVQA">code</a> / <a href="data/groundvqa.bib">bibtex</a>
              <p>Simultaneous query grounding and answering in long, egocentric videos.</p>
            </td>
          </tr>

          <!-- Linker -->
          <!-- <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
            </td>
            <td width="70%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/10416989">
                <papertitle>Linker: Learning Long Short-term Associations for Robust Visual Tracking</papertitle>
              </a>
              <br>
              Zizheng Xun*, <b>Shangzhe Di</b>*, Yulu Gao, Zongheng Tang, Gang Wang, Si Liu, Bo Li
              <br>
              In <em>IEEE Transactions on Multimedia (TMM)</em>, 2023.
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10416989">paper</a>
              <p></p>
              <br>
            </td>
          </tr> -->

          <!-- CMT -->
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle;text-align:center;">
              <img src="images/MM21_CMT.jpg" width="225">
            </td>
            <td width="70%" valign="middle">
              <a href="https://wzk1015.github.io/cmt/">
                <papertitle>Video Background Music Generation with Controllable Music Transformer</papertitle>
              </a>
              <br>
              <b>Shangzhe Di</b>*, Zeren Jiang*, Si Liu, Zhaokai Wang, Leyan Zhu, Zexin He, Hongming Liu, Shuicheng Yan
              <br>
              In <em>ACM MM</em>, 2021. <font color="red"><strong>(Best Paper Award)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2111.08380v1">paper</a> / <a href="https://wzk1015.github.io/cmt/">project page</a> / <a href="https://github.com/wzk1015/video-bgm-generation"> code </a> / <a href="https://colab.research.google.com/github/wzk1015/video-bgm-generation/blob/develop/CMT.ipynb"> colab notebook </a> / <a href="data/cmt.bib">bibtex</a>
              <p>The first satisfying method for video background music generation.</p>
            </td>
          </tr>

<!--           <tr onmouseout="tryon_stop()" onmouseover="tryon_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tryon_image'>
                  <img src='images/tryon_2.jpg' width="160"></div>
                <img src='images/tryon_1.jpg' width="160">
              </div>
              <script type="text/javascript">
                function tryon_start() {
                  document.getElementById('tryon_image').style.opacity = "1";
                }

                function tryon_stop() {
                  document.getElementById('tryon_image').style.opacity = "0";
                }
                tryon_stop()
              </script>
            </td>
            <td width="75%" valign="middle">
              
              <papertitle>Powering Image-based Virtual Try-on with Structural Information and Cascade Warping</papertitle>
            
              <br>
              <u>Shangzhe Di</u>, Junjie Chen, Songtao Zhao
              <br>
              Rejected by <em>CVPR</em>, 2021
              <br>
              <p>Powering image-based virtual try-on with high-quality structural information and coarse-to-fine clothes warping. </p>
            </td>
          </tr> -->
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Honors and Awards</heading>
              <div style="line-height:25px">
                <p>
                  <li>
                    <a href="data/MM_Best.jpeg">Best Paper Award, ACM MM 2021</a><br />
                  <li>
                    <a href="data/IJCAI_Best_Video.pdf">Best Video Award, IJCAI 2021 Video Competition</a><br />
                  <!-- <li>
                    <a href="https://anti-uav.github.io/leaderboard2/">2nd Place, AntiUAV Competition at ICCV 2021</a><br />
                  <li>
                    <a href="https://www.aicrowd.com/challenges/airborne-object-tracking-challenge/leaderboards">2nd Place, Airborne Object Tracking Challenge at ICCV 2021</a><br /> -->
                  <li>
                    First Prize Scholarship x 2 (Top 10%), Beihang University, 2019 & 2021<br />
                  <li>
                    Full Scholarship for Exchange Program, China Scholarship Council, 2019<br />
                  <li>
                    Special Prize Scholarship (Top 3%), Beihang University, 2018<br />
                </p>
              </div>
            </td>
          </tr>
        </table>

        <br>
        The website template is borrowed from <a href="https://jonbarron.info">here</a>.

      </td>
    </tr>
  </table>

<p style="text-align:center;">
  <a href="https://www.easycounter.com/">
    <img src="https://www.easycounter.com/counter.php?dsz6223042" border="0" alt="Web Counter">
  </a>
</p>

</body>

</html>
